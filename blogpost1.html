<!DOCTYPE html>
<html lang="en">
<head>
<style>
body {font-family: monospace}
</style>

<title>no turning back</title>
<link rel="icon" type="image/x-icon" href="img/favicon-david.ico">

</head>
<body>

<h1>Building a python ray tracer from scratch</h1>
<input type="image" id="smoothshading" src="blogpost1_images/smoothshading.PNG" width="400" height="400" alt="smoothshading">
<br/><br/>
Check out the project on <a style="color:rgb(61, 61, 255);" href="https://github.com/grantcary/little-engine/tree/main">GitHub</a>!
<br/><br/>
<big><big><big><big>Preface</big></big></big></big>
<p>
    This post is hopfully an insight into my first complex project, or should I say a relatively simple project turned complex. First off, the ray tracer 
    I built is super basic, you can't even rotate the objects in the scene yet. It is in no way fully fledged. As of right now, it can only load .OBJ files 
    into a scene, and render it with reflections and shadows, with shadows currently being the main bottleneck. However, I definitely want to expand on this 
    project and add more features over time to get it closer to photo realistic.
</p>

<big><big><big><big>Starting off</big></big></big></big>
<p>
    Never having done a project like this has really been a learning experience. For about a year now, I've been mostly in consumption learning mode.
    Doing a few small projects here and there, but nothing of great difficulty and not moving anywhere. I knew I needed a challenge, but didn't know 
    what kind of project I wanted to start, everything seemed so daunting, an insurmountable wall of knowledge. At first, I was trying to wrap my head 
    around machine learning concepts for a project I started called <a style="color:rgb(61, 61, 255);" href="https://github.com/grantcary/saga-color">saga color</a> 
    (still a wip), a tool to automatically transfer color grading from one image to another, retaining things like tone and accounting for things like 
    lighting difference between scenes, a type of style transfer. Not able to fully grasp the concepts behind what I wanted to do, I started to look else where. 
</p><p>
    <a style="color:rgb(61, 61, 255);" href="https://youtu.be/Qz0KTGYJtUk">This</a> video by  
    <a style="color:rgb(61, 61, 255);" href="https://www.youtube.com/@SebastianLague">Sebastian Lague</a> popped into my recommended feed and gave a simple 
    entry into the basics of ray tracing, inspiring me to designing an engine myself. Finally I felt like there was now a project I could sink my teeth into. 
    The core concepts are extremely simple, I have read explainers on this topic before, having long time experience in 3D software like Blender, time to get started. 
    Only, what language to choose?
</p>

<br/><br/>
<big><big><big><big>Why python?</big></big></big></big>
<p>
    Honest and simple truth, it is the language I know the best. I went into it knowing the speed barriers I would have to live with and manage. 
    I also felt like writing a ray tracer in a language like C++ is kinda cheating. There are a ton of resources to pull from and code snippets to copy paste, 
    building the system would be relatively easy. I needed a way to learn quickly. Most of the C++ examples are, logically, using loops. I decided to take on the 
    challenge of translating functions based on loops into functions based on vectorized numpy operations, which turns out is harder than it seems.
</p>

<br/><br/>
<big><big><big><big>Meshes</big></big></big></big>
<p>
    For the first month into development, I was headed into a much different direction than where I ended up. Before even deciding to go all in on numpy vectorization, 
    I was trying to make each individual vertex, face and normal an instance of their own classes, adding a ton of complexity issues and structural hurdles moving forward. 
    Once I realized I wouldn't get far if I kept going the direction, I gave up on that pretty fast.
    <br/><br/>
    For the basis of everything, I wanted an extremely simple class with little to no frills. The mesh class only stores what is needed: vertices, faces and normals.
    All of which are 2D numpy arrays. I used face-vertex mesh as the data representation.
    <br/><br/>
    <input type="image" onclick="window.location.href='https://en.wikipedia.org/wiki/Polygon_mesh';" id="face-vertex mesh" src="blogpost1_images/Mesh_fv.jpg" width="700" height="443" alt="face-vertex mesh">
    <br/><br/>
    However, I didn't implement the standard approach. In the example above, the vertex list stores the indices of every face that uses that given vertex. I realized this reference 
    to the face list wasn't needed in my application and just added unnecessary complexity. 
    <br/></br>
    Implementing a triangulation algorithm was my next step. Decided not to get into delaunay triangulation because it seemed to be above my skill level, and also none of 
    my test objects had holes in them. Choosing the easiest one to get my head around, I found <a style="color:rgb(61, 61, 255);" href="https://www.youtube.com/watch?v=QAdfkylpYwc">this</a> 
    great video explaining how it worked.
    <br/></br>
    Finally, I created an Object class that would act as a wrapper for the raw mesh data, adding useful data like color, reflectivity, ior and actaully loading the data from .OBJ files.
</p>

<br/><br/>
<big><big><big><big>First steps</big></big></big></big>
<p>
    The first iteration of the actual ray tracing algorithm was simple, a ray triangle intersection function using MÃ¶ller-Trumbore's algorithm, and a trace function that 
    looped through all triangles in the scene. If a triangle was hit, it would add a predetermined RGB value to an image array that was initialized with every value being 
    a background color. The resulting image was a silhouette of the default cube. Then, altering the code to add the individual object's unique color to the array resulted 
    in this first render.
    <br/><br/>
    <input type="image" id="first render" src="blogpost1_images/firstrender.PNG" width="400" height="400" alt="first render">
</p>

<br/><br/>
<big><big><big><big>Toon shading</big></big></big></big>
<p>
    Creating a toon shader was an unintentional encounter. One that I didn't quite get. When I started working on the shadow function, I expected the output to be closer to 
    photorealism than not. Though, I got some cool results from it.
    <br/><br/>
    <input type="image" id="shaded" src="blogpost1_images/shaded.PNG" width="400" height="400" alt="shaded">
    <input type="image" id="toon shaded" src="blogpost1_images/toonshaded.PNG" width="400" height="400" alt="toon shaded">
    <br/><br/>
    When I wrote the ray triangle intersection function, it was designed to only take one triangle, one ray origin, and an array of ray directions. Luckily, when testing how 
    it would work with an array of origins, it worked without any errors. But the resulting data was not what I was expecting. It ended up spitting out an "rays * rays" shaped 
    matrix. I tried gathering any useful data from the matrix using masks and filters, then I finally landed on np.diagonal. Finally, useful data.
</p>

<br/><br/>
<big><big><big><big>Reflections</big></big></big></big>
<p>
    This is where I was actually blown away by something I created. This is the image I had in my head when I started work on the shadow function. Finally an image that looks... 
    well... 3D! And the function was simple, only taking up 5 lines of code.
    <br/><br/>
    <input type="image" id="smooth shading" src="blogpost1_images/smoothshading.PNG" width="400" height="400" alt="smooth shading">
    <br/><br/>
    This is also where I started seeing the massive bottleneck. Looping the main code to calculate the scene, it was apparent that the shadow function was, horrendously slow. 
    Even with creating a mask to filter out non-hit rays before passing it throught the function for the next pass, it is still extremely slow on the first pass. The issue 
    arises in the ray triangle intersection function. Instead of needing to calculate only one ray origin for all rays, there are now the same amount of origins as there are 
    rays. So as it gets calculated, the resulting matrix size of the intersections is now "rays * rays". After that, I only end up taking the diagonal of the matrix, which 
    only accounts for 0.01% of the total elements calculated. Now do that operation for every triangle in the scene. Let's say that an object with 1000 triangles is scaled 
    up so that it fills the entire camera frame and every primary ray hits, the render resolution would be set to 100 by 100. Just one pass of the shade calcuation would be 
    "(100 * 100)**2 * 1000" or 100,000,000,000 or one hundred billion calculations, but only 10,000,000 or ten million useful values. Now, I have tried rewriting this in 
    pure python with for loops, skipping the unnecessary calculation, and it was still slower.
    <br/><br/>
    Running into this issue has given me some awareness on a misconception I had going into this. And that was that numpy can fix everything. Now being in a place where both 
    python's speed and numpy's speed are working against me, has been a bit tricky. Not a whole lot of options left to run to.
    <br/><br/>
    Long story short, this is an active problem needing to be solved, and the next section is about my attempt at solving it.
</p>

<br/><br/>
<big><big><big><big>Bounding Volume Hierarchy</big></big></big></big>
<p>
    Exhausted, I turned to ChatGPT for an answer. I asked it how it would speed up my code, it recommended an acceleration data structure bounding volume hierarchy. I had 
    never heard of an acceleration structure before. Digging deeper, I found that the basis of the speedup was based on simple ray-cube intersection. Looking at examples, 
    I noticed that bounding boxes weren't just placed around entire objects, they were also placed around groupings of triangles that made up the object. Another question 
    to ChatGPT asking what splitting up meshes into smaller groups was called, and that leads us to the next section.
</p>

<br/><br/>
<big><big><big><big>Meshlets</big></big></big></big>
<p>
    I found out that there isn't a lot of documentation on the internet on how to generate meshlets. I came across posts on forums explaining that it is a very fresh field and 
    that besides Nvidia or Epic, there was no public example of how the algorithm would be structured, which didn't make sense since I had seen examples of it used. Then, 
    I saw a post linking to a developer who built <a style="color:rgb(61, 61, 255);" href="https://github.com/zeux/meshoptimizer">this</a>  library with the functionality 
    I had been looking for. I read through the file many times to get a grasp on how it worked, and to get an idea about how to implement it in python.
    <br/><br/>
    The actual implementaion was very hard, mostly for the fact that the C++ implementation was working with offsets for arrays and other lower level tasks that I didn't have 
    to think about. It was hard to see what I didn't actually need until I fully coded it.
    <br/><br/>
    My first iteration of the meshlet builder was O(n**2). Some objects like the Utah Teapot was left running for about 8 hours over night and still wasn't even 
    close to finishing. After assessing the code, it ended up being the KDTree that was being searched for the closest triangle to be added to the meshlet. As more and more 
    triangles became unavailable, the KDTree would have to extend it's search, and it would keep extending the search until it found a viable triangle. So, instead of building 
    one KDTree out side of the loop, a new KDTree would be generated for every iteration of the loop with updated available triangles. Now the teapot generates it's meshlets 
    in about 5 seconds.
    <br/><br/>
    So, finally after building two very complex data structures to help speed up my algoritms, surely there would be an instant improvement once implemented. Well, first tests 
    using the BVH to cull out rays that won't be hitting a light source before the shade function resulted in some weird visual bugs and a minor speed improvement.
    <br/><br/>
    <input type="image" id="glitch" src="blogpost1_images/bvhcullingglitch.PNG" width="400" height="400" alt="glitch">
    <br/><br/>
    There are a lot more tests I can do to try and speed things up. Even if I don't end up using this in the actual working code, it was a great learning experience on fairly 
    complex algoritms like these.
</p>

<br/><br/>
<big><big><big><big>The future is hybrid?</big></big></big></big>
<p>
    I've done some minor searching into a possible hybrid approach, however it seems to be quite complicated to be switching back and forth between the two types of rendering. 
    Although, I will be conducting some experiments on its viability in the future.
</p>

<br/><br/>
<big><big><big><big>The end (for now)</big></big></big></big>
<p>
    This was a fun project, I'm still going to be working on it here and there. The main focus right now with this project is to make it more editable. Right now I feel like 
    every piece of code is intertwined with one another to the point where it is hard to experiment and add new features. However, I feel it is time to start work on new projects.
</p>